# PPO training configuration
algorithm: "ppo"

# Network architecture
network:
  hidden_dims: [256, 256]
  activation: "relu"
  orthogonal_init: true
  
# PPO hyperparameters
ppo:
  learning_rate: 3.0e-4
  gamma: 0.99
  gae_lambda: 0.95
  clip_epsilon: 0.2
  vf_coef: 0.5
  ent_coef: 0.01
  max_grad_norm: 0.5
  
  # Batch sizes
  num_envs: 8
  num_steps: 128
  num_minibatches: 4
  update_epochs: 4
  
# Training schedule
training:
  total_timesteps: 1_000_000
  eval_frequency: 10_000
  checkpoint_frequency: 50_000
  log_frequency: 1_000
  
# Experiment tracking
wandb:
  project: "drone-swarms"
  entity: null
  mode: "online"  # or "disabled"
